### scipt with methods for tokenizing and screening the documents ###
require("tokenizers")
require("tidyverse")
require("stringr")

# Function to tokenize the document and calulate the frequencies
#' Function to tokenzie the document and calculate the frequencies of each unique token
#' 
#' This function takes a string as an argument and returns a data frame containing the
#' unique tokens and their frequencies, organized so that the most frequent
#' ones are on top
#' @param string, the string which you would like tokenized
#' @keywords tokenize, word frequency, token
#' @export

tokenize <- function(string) {
  if (!requireNamespace("tokenizers", quietly = TRUE)) {
    stop("tokenizers is needed for this function to work. Please install it.",
         call. = FALSE)
  }
  if (!requireNamespace("tidyverse", quietly = TRUE)) {
    stop("tidyverse is needed for this function to work. Please install it.",
         call. = FALSE)
  }
  words <- tokenize_words(string)
  wordsFreq <- table(words)
  wordsFreq <- data_frame(word = names(wordsFreq), count = as.numeric(wordsFreq))
  wordsFreq <- arrange(wordsFreq, desc(count))
  return(wordsFreq)
}

# Function to return the number of occurances of a word in 
# the given text
#' Function which returns the count of a word given the data frame
#' generated by the corpus level word frequency function getFromFolderWF
#' and the word requested
#' 
#' @param wordCountDataFrame, the unique tokens and their frequencies
#' @param searchString, the string you would like to know the frequency of
#' @keywords tokenize, word frequency, token, get count
#' @export

getCount <- function(wordCountDataFrame, searchString) {
  if (length(wordCountDataFrame) > 2) {
    count <- 0
    for (i in 1:length(wordCountDataFrame)) {
      rowNumbers <- c()
      # the data structure holding the tokens and their frequencies
      # is organized so that even columns hold the frequencies
      # and odd columns hold the tokens
      if (i %% 2 == 0) {
        rowNumbers <- c(1:length(wordCountDataFrame[[i]]))
        names(rowNumbers) <- wordCountDataFrame[[i-1]]
        # If the search string is not in the tokens, this condition
        # will progress to the next iteration of the for loop
        if (!(searchString %in% names(rowNumbers))) {
          next
        }
        index <- rowNumbers[[searchString]]
        tempCount <- wordCountDataFrame[[i]][index]
        if (length(tempCount) > 0) {
          count <- count + tempCount
        }
      }
    }
    return(count)
  }
  else {
    # Here as well, the odd numbered columns are tokens
    # and the even numbered columns are frequencies
    rowNumbers <- c(1:length(wordCountDataFrame[[2]]))
    names(rowNumbers) <- wordCountDataFrame[[1]]
    if (!(searchString %in% names(rowNumbers))) {
      return(0)
    }
    index <- rowNumbers[searchString]
    return(wordCountDataFrame[[2]][index])
  }
}

#' function which returns a word frequency data frame from the portion
#' of a text which equal in length to the requested sample size
#' 
#' This function takes a string as an argument and returns null if the
#' string has less than 300 tokens or a word frequency data frame
#' of the first 300 token if it is 300 tokens or longer
#' @param text, the text you would like tokenized
#' @param minMaxWordCount, no documents with less tokens than indicated will be accepted and all documents longer than the spefified count will be cropped Defaults to 300
#' @keywords tokenize, word frequency, token, sample size
#' @export

# Return a word-frequency data structure with all the unique
# tokens and their rate of occurance
getFixedSampleSizeWF <- function(text, minMaxWordCount = 300) {
  string <- make300(text, minMaxWordCount)
  # tokenize the document
  if(!is.null(string)) {
    wordFreq <- tokenize(string)
    return(wordFreq)
  }
}
# Return a unprocessed string
#' Function which returns a string that is the requested size
#' 
#' This function takes a string as an argument and returns null if the
#' string has less than 300 tokens or the first 300 tokens if 
#' it is 300 tokens or longer
#' @param text, the text you would like tokenized
#' @param minMaxWordCount, no documents with less tokens than indicated will be accepted and all documents longer than the spefified count will be cropped Defaults to 300
#' @keywords tokenize, word frequency, token, sample size
#' @export

getFixedSampleSizeS <- function(text, minMaxWordCount = 300) {
  return(make300(text, minMaxWordCount))
}
# Function to return a 300 token long string of a documnet
# returning na if the document is less than 300 tokens long
#' Function which cuts strings to the requested sample size
#' and returns null if they are shorter than the sample size
#' 
#' This function takes a string as an argument and returns null if the
#' string has less than 300 tokens or a word frequency data frame
#' of the first 300 token if it is 300 tokens or longer
#' @param text, the text you would like tokenized
#' @param minMaxWordCount, no documents with less tokens than indicated will be accepted and all documents longer than the spefified count will be cropped Defaults to 300
#' @keywords sample size
#' @export

make300 <- function(text, minMaxWordCount) {
  if (!requireNamespace("stringr", quietly = TRUE)) {
    stop("stringr is needed for this function to work. Please install it.",
         call. = FALSE)
  }
  wordCount <- sapply(gregexpr("[A-z]\\W+", text), length)
  wordCount <- sum(wordCount)
  if(wordCount < minMaxWordCount) {
    return(NULL)
  }
  # tokenize the document
  tokens <- str_split(text, "[:blank:]")
  # Make an empty string to hold the 300 tokens
  string <- ""
  # Make a counter 
  counter <- 0
  for (i in 1:length(tokens)) {
    for (j in 1:length(tokens[[i]])) {
      if (counter <= minMaxWordCount) {
        string <- paste(string, tokens[[i]][j])
        counter <- counter + 1
      }
    }
  }
  return(string)
}
# Reads in all files from a folder, then passes to make300 with
# the arguments fed to it by the user.  WF returns word frequency
#' Function which takes a folder as an argument and returns a vector
#' with the unique tokens for each document and their frequencies
#' 
#' This function takes a path to a folder as an argument.  It then
#' excludes all documents in the folder which have less than 300 
#' tokens and process the rest.  The output is a data structure which
#' can be iterated through length-wise.  Odd indexes in the data structure
#' return lists which hold the unique tokens of an individual document,
#' even indexes hold the frequencies
#' @param pathToFolder, the folder containing your corpus
#' @param minMaxWordCount, no documents with less tokens than indicated will be accepted and all documents longer than the spefified count will be cropped Defaults to 300
#' @keywords tokenize, word frequency, token, sample size
#' @export

getFromFolderWF <- function(pathToFolder, minMaxWordCount = 300) {
  # Get a list of the files
  files <- list.files(pathToFolder, pattern = "*.txt")
  # Make a counter
  acceptedDocuments <- 0
  # Make a list of data frames
  wordFrequencies <- c()
  # Make a console progress bar
  print("Files read:")
  pb <- txtProgressBar(min = 0, max = length(files), initial = 1, char = "=", width = NA, style = 3, file = "")
  for (i in 1:length(files)) {
    path <- paste(pathToFolder, files[i], sep = "")
    con <- file(path) 
    document <- tolower(readLines(con))
    close(con)
    # make sure there are no non-ASCII characters
    document <- iconv(document, "latin1", "ASCII", sub="")
    wordFrequency <- getFixedSampleSizeWF(document, minMaxWordCount)
    if(!is.null(wordFrequency)) {
      acceptedDocuments <- acceptedDocuments + 1
      wordFrequencies <- c(wordFrequencies, wordFrequency)
      setTxtProgressBar(pb, i)
    }
  }
  print("Number of accepted documents: ")
  print(acceptedDocuments)
  print("Out of: ")
  print(length(files))
  return(wordFrequencies)
}
# Gets vector of strings instead of a word frequency data structure
#' Function which takes a folder as an argument and returns a vector
#' with the unique tokens for each document and their frequencies
#' 
#' This function takes a path to a folder as an argument.  It then
#' excludes all documents in the folder which have less than 300 
#' tokens and process the rest.  The output is a vector of 
#' strings containing each acceptable, cropped document
#' @param pathToFolder, the folder containing your corpus
#' @param minMaxWordCount, no documents with less tokens than indicated will be accepted and all documents longer than the spefified count will be cropped Defaults to 300
#' @keywords tokenize, word frequency, token, sample size
#' @export

getFromFolderString <- function(pathToFolder, minMaxWordCount = 300) {
  # Get a list of the files
  files <- list.files(pathToFolder, pattern = "*.txt")
  # Make a vector of strings
  documents <- c()
  # Make a counter
  acceptedDocuments <- 0
  # Make a console progress bar
  print("Files read:")
  pb <- txtProgressBar(min = 0, max = length(files), initial = 1, char = "=", width = NA, style = 3, file = "")
  for (i in 1:length(files)) {
    path <- paste(pathToFolder, files[i], sep = "")
    con <- file(path) 
    document <- tolower(readLines(con))
    close(con)
    tempDoc <- getFixedSampleSizeS(document, minMaxWordCount)
    if(!is.null(tempDoc)) {
      acceptedDocuments <- acceptedDocuments + 1
      documents <- c(documents, tempDoc)
      setTxtProgressBar(pb, i)
    }
  }
  print("Number of accepted documents: ")
  print(acceptedDocuments)
  print("Out of: ")
  print(length(files))
  return(documents)
}
#' Function which takes a path to a folder and returns one data frame
#' 
#' Function which takes a path to a folder and returns
#' a single data frame of the tokens and their frequencies
#' @param pathToFolder, the folder containing your corpus
#' @param minMaxWordCount, no documents with less tokens than indicated will be accepted and all documents longer than the spefified count will be cropped Defaults to 300
#' @keywords tokenize, word frequency, token, sample size
#' @export

getOneDataFrame <- function(pathToFolder, minMaxWordCount = 300) {
  # Get a list of the files
  files <- list.files(pathToFolder, pattern = "*.txt")
  # Make a counter
  acceptedDocuments <- 0
  # Make one large document
  documents <- ""
  # Make a console progress bar
  print("Files read:")
  pb <- txtProgressBar(min = 0, max = length(files), initial = 1, char = "=", width = NA, style = 3, file = "")
  for (i in 1:length(files)) {
    path <- paste(pathToFolder, files[i], sep = "")
    con <- file(path) 
    document <- tolower(readLines(con))
    close(con)
    # make sure there are no non-ASCII characters
    document <- iconv(document, "latin1", "ASCII", sub="")
    document <- getFixedSampleSizeS(document, minMaxWordCount)
    if(!is.null(document)) {
      acceptedDocuments <- acceptedDocuments + 1
      documents <- paste(documents, document)
      setTxtProgressBar(pb, i)
    }
  }
  wordFrequencies <- tokenize(documents)
  print("Number of accepted documents: ")
  print(acceptedDocuments)
  print("Out of: ")
  print(length(files))
  return(wordFrequencies)
}
